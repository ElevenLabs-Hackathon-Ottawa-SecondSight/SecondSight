ðŸ¦… OPERATION: SECOND SIGHT (The Definitive Masterplan)
Event: ElevenLabs Worldwide Hackathon (Global Track)
Objective: Build a Multimodal Reality-OS that Sees, Knows, and Remembers.
Winning Criteria: Score 5/5 on Technical Complexity (Orchestration) & Innovation (Accessibility).

1. ðŸ—ï¸ THE ARCHITECTURE (How it Works)
We are building a Single Page App (SPA) where the "Backend" is mostly serverless API routes generated by Bolt.
Auth (The Gatekeeper): Clerk ensures only logged-in users access the agent.
The Brain (Orchestrator): ElevenLabs Agent listens and decides which tool to use.
The Eyes (Vision): LLM call via an API route.
The Knowledge (Search): Tavily via an API route.
The Memory (Database): Clerk User Metadata. We hijack the user profile to store JSON data (memories), avoiding the need for an external database like Supabase.

2. ðŸŽ’ PRE-GAME INVENTORY (The 6 Keys)
Assign 1 person to collect these into a text file BEFORE the timer starts.
ElevenLabs API Key: (From elevenlabs.io -> Profile -> API Key).
Clerk Publishable Key: (pk_test_... from dashboard.clerk.com).
Clerk Secret Key: (sk_test_... from dashboard.clerk.com).
LLM API Key: (sk-... from platform.openai.com).
Tavily API Key: (tvly-... from tavily.com).
GitHub Account: Logged in and ready to create a repo.

3. â±ï¸ THE 3-HOUR EXECUTION STEPS
ðŸ”´ PHASE 1: THE BRAIN SETUP (0:00 - 0:20)
Owner: The Prompt Engineer
Goal: Configure the Agent's personality and define the tools it thinks it has.
Create Agent: Go to ElevenLabs Dashboard -> Conversational AI -> Create Agent.
Name: Second Sight Orchestrator.
System Prompt: (Copy Exact Text)
codeText
You are Second Sight, a multimodal OS for the blind.
You are NOT a chatbot. You are an Orchestrator.

YOUR TOOLS:
1. Visuals: If the user asks "What is this?" or shows an object -> CALL `getVisualContext`.
2. Knowledge: If the user asks for facts/news -> CALL `webSearch`.
3. Memory (Write): If the user says "Remember [X]" -> CALL `saveMemory`.
4. Memory (Read): If the user says "What did I save?" or needs context -> CALL `readMemory`.

BEHAVIOR:
- Be concise.
- Do not narrate "I am looking". Just trigger the tool.
- If a tool fails, apologize briefly.


Client Tools: Go to "Client Tools" -> "Add Tool". Create these 4 exact definitions:
Tool A: 
Description: Analyze the camera view.
Parameters: {} (Empty JSON).


Tool B: 
Description: Search the web.
Parameters:
codeJSON
{ "type": "object", "properties": { "query": { "type": "string" } }, "required": ["query"] }




Tool C: 
Description: Save a fact to long-term memory.
Parameters:
codeJSON
{ "type": "object", "properties": { "fact": { "type": "string" } }, "required": ["fact"] }




Tool D: 
Description: Retrieve saved memories.
Parameters: {} (Empty JSON).




Action: Copy the Agent ID.

ðŸŸ  PHASE 2: THE SKELETON (0:20 - 1:00)
Owner: The Bolt Architect (Driver)
Goal: Generate the App, Auth, and UI in one massive prompt.
Open Bolt.new.
Paste this Master Prompt:
"Build a mobile-first Next.js application using Tailwind CSS, Lucide React icons, and Framer Motion.
1. THE UI LAYOUT:
Background: A full-screen live <video> element (User's Camera). Muted, autoplay, playsinline.
Overlay: A 'Glassmorphism' HUD (Heads Up Display) on top of the video.
Controls: A large, pulsing circular button at the bottom center to toggle the ElevenLabs Agent.
Feedback: A 'Status Chip' in the top-right corner displaying text: 'Idle', 'Listening', 'Thinking...'.
2. AUTHENTICATION (CRITICAL):
Install @clerk/nextjs.
Wrap the root layout in <ClerkProvider>.
In the main page, use <SignedIn> to show the Camera View and <SignedOut> to show a 'Login with Clerk' button.
The app must not show the camera unless the user is logged in.
3. AGENT INTEGRATION:
Use @11labs/react and the useConversation hook.
Connect to the Agent ID (provided in env).
4. BACKEND SCAFFOLD:
Create 4 API Route files in /app/api/ (Empty logic for now):
POST /api/vision
POST /api/search
POST /api/memory/save
POST /api/memory/read


STYLE:
Cyberpunk aesthetic. Dark mode. Neon Green accents."
Configure Env Vars:
Inside Bolt, create .env.local immediately:
codeEnv
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=...
CLERK_SECRET_KEY=...
NEXT_PUBLIC_ELEVENLABS_AGENT_ID=...
OPENAI_API_KEY=...
TAVILY_API_KEY=...



ðŸŸ¡ PHASE 3: THE NERVOUS SYSTEM (1:00 - 2:15)
Owner: Team Swarm
Goal: Implement the logic. This is where we wire Clerk for Memory and OpenAI for Vision.
Step A: Implement Vision (/app/api/vision/route.ts)
Prompt Bolt:
"Update /app/api/vision. It accepts { image: base64 }. Use OpenAI SDK (gpt-4o) to analyze the image. System prompt: 'Describe this image for a blind person. Identify text and objects.' Return JSON { text: '...' }."
Step B: Implement Search (/app/api/search/route.ts)
Prompt Bolt:
"Update /app/api/search. Accept { query }. Use Tavily API to search. Return top 2 results summary."
Step C: Implement Memory SAVE (/app/api/memory/save/route.ts)
Prompt Bolt:
"Update /app/api/memory/save.
Use auth() from @clerk/nextjs/server to get the userId. Return 401 if missing.
Accept { fact }.
Use clerkClient.users.getUser to fetch current publicMetadata.
Append fact to the memories array in metadata.
Use clerkClient.users.updateUserMetadata to save it."
Step D: Implement Memory READ (/app/api/memory/read/route.ts)
Prompt Bolt:
"Update /app/api/memory/read.
Check userId.
Fetch user metadata via clerkClient.
Return the memories array as a JSON string."
Step E: Wire the Client Tools (The Glue)
Prompt Bolt:
"Update the main component. Define the clientTools object for the ElevenLabs hook:
getVisualContext:
Action: Set Status to 'Looking...'. Capture video frame to base64. POST to /api/vision.
UX: Play a 'beep' sound immediately.
Return: The text description.


webSearch:
Action: Set Status to 'Searching...'. POST to /api/search.
Return: The search result.


saveMemory:
Action: Set Status to 'Saving...'. POST to /api/memory/save.
Return: 'Memory saved.'


readMemory:
Action: Set Status to 'Recalling...'. POST to /api/memory/read.
Return: The list of memories."



ðŸŸ¢ PHASE 4: POLISH & DEPLOY (2:15 - 2:40)
Owner: Asset Manager
Sponsor Check:
Create a Repo on GitHub.
Install CodeRabbit on that Repo (Essential for prize).
Push code to GitHub.
Deploy:
Deploy to Vercel.
CRITICAL: Go to Vercel Project Settings -> Environment Variables. Paste all keys from .
If Vercel fails: Don't panic. Use ngrok http 3000 for the demo.

ðŸ”µ PHASE 5: THE DEMO (2:40 - 3:00)
Owner: Presenter
The Setup:
Laptop connected to projector (if live) or OBS (if video).
App running on HTTPS (Vercel or Ngrok).
Login: Sign in with Clerk before the recording starts.
The Script (Strict Adherence):
[0:00] Hook: "For the visually impaired, context is everything. Second Sight is a Reality OS that sees, knows, and remembers."
[0:15] Vision (Multimodal):
Action: Hold up a distinct object (e.g., Apple).
Say: "What am I holding?"
AI: "I see a red apple."
[0:30] Search (Orchestration):
Say: "Is this a good snack for a diabetic?"
Visual: Status Chip changes to 'Searching...'.
AI: "Apples have a low glycemic index, making them a safe choice."
[0:45] Memory (Clerk):
Say: "Remember that I like green apples better."
Visual: Status Chip changes to 'Saving...'.
AI: "Saved preference."
[1:00] Recall (Clerk):
Say: "What did I say about apples?"
AI: "You prefer green apples."
[1:15] Closing: "Built with Bolt, ElevenLabs, Clerk, and CodeRabbit."

ðŸ†˜ TROUBLESHOOTING
"My phone camera isn't working!"
Reason: You are using http://localhost.
Fix: You MUST use https. Use ngrok (ngrok http 3000) or the Vercel link.
"The Agent just says 'I don't know' without looking."
Reason: It forgot it has tools.
Fix: Update System Prompt: "You MUST use the getVisualContext tool if the user asks about an object."
"Clerk says Unauthorized."
Reason: API route isn't seeing the session.
Fix: Ensure you are passing the session cookie. (Bolt usually handles this). If stuck, hardcode the userId in the API route just to survive the demo.

